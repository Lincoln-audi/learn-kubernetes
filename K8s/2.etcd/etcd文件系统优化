1.使用 NVMe ssd
etcd 自身 = etcd 程序 + 其运行环境。早期 etcd 服务器使用的磁盘是 SATA 盘，经过简单地测试发现 etcd 读磁盘速率非常慢，老板豪横地把机器鸟枪换炮 —- 
升级到使用了 NVMe SSD 的 f53 规格的机器：etcd 使用 NVMe ssd 存储 boltdb 数据后，随机写速率可提升到 70 MiB/s 以上。



2.使用 tmpfs
NVMe ssd 虽好，理论上其读写极限性能跟内存比还是差一个数量级。我们测试发现使用 tmpfs【未禁止 swap out】替换 NVMe ssd 后，etcd 在读写并发的情况下性能仍然能提升 20% 之多。考察 k8s 各种数据类型的特点后，考虑到 event 对数据的安全性要求不高但是对实时性要求较高的特点，我们毫不犹豫的把 event etcd 集群运行在了 tmpfs 文件系统之上，将 k8s 整体的性能提升了一个层次。

3.磁盘文件系统
磁盘存储介质升级后，存储层面能够进一步做的事情就是研究磁盘的文件系统格式。目前 etcd 使用的底层文件系统是 ext4 格式，其 block size 使用的是默认的 4 KiB。我们团队曾对 etcd 进行单纯的在单纯写并行压测时发现，把文件系统升级为 xfs，且 block size 为 16 KiB【在测试的 KV size 总和 10 KiB 条件下】时，etcd 的写性能仍然有提升空间。
但在读写并发的情况下，磁盘本身的写队列几乎毫无压力，又由于 etcd 3.4 版本实现了并行缓存读，磁盘的读压力几乎为零，这就意味着：继续优化文件系统对 etcd 的性能提升空间几乎毫无帮助。自此以后单节点 etcd scale up 的关键就从磁盘转移到了内存：优化其内存索引读写速度。

4.磁盘透明大页
在现代操作系统的内存管理中，有 huge page 和 transparent huge page 两种技术，不过一般用户采用 transparent huge page 实现内存 page 的动态管理。在 etcd 运行环境，关闭 transparent huge page 功能，否则 RT 以及 QPS 等经常性的监控指标会经常性的出现很多毛刺，导致性能不平稳。
